








import numpy as np
import matplotlib.pyplot as plt
import scipy.spatial.distance as sd
from scipy.io import loadmat
import os
from helper import build_similarity_graph, label_noise
from helper import build_laplacian, build_laplacian_regularized
from helper import plot_classification
from helper import mask_labels


"""
Define parameters for HFS
"""
params = {}

# regularization parameter (gamma)
params['laplacian_regularization'] = 0.0

# the sigma value for the exponential (similarity) function, already squared
params['var'] = 1.0

# Threshold eps for epsilon graphs
params['eps'] = None

# Number of neighbours k for k-nn. If zero, use epsilon-graph
params['k'] = 5

# String selecting which version of the laplacian matrix to construct.
# 'unn':  unnormalized, 'sym': symmetric normalization, 'rw':  random-walk normalization 
params['laplacian_normalization'] = 'unn'

# Coefficients for C matrix for soft HFS
params['c_l'] = 3.0
params['c_u'] = 1.0


def compute_hfs(L, Y, soft=False, verbose=False, **params):
    """
    TO BE COMPLETED

    Function to perform HFS (hard or soft!).

    Parameters
    ----------
    L : array
        Graph Laplacian, (n x n) matrix (regularized or not)
    Y : array
        (n, ) array with nodes labels [0, 1, ... , num_classes] (0 is unlabeled)
    soft : bool
        If True, compute soft HFS. Otherwise, compute hard HFS.

    Returns
    --------
        Labels, class assignments for each of the n nodes
    """

    num_samples = L.shape[0]
    Cl = np.unique(Y)
    num_classes = len(Cl)-1

    """
    Build the vectors:
    y = (n x num_classes) target vector 
    l_idx = shape (l,) vector with indices of labeled nodes
    u_idx = shape (u,) vector with indices of unlabeled nodes
    """
    
    # build target vector
    y = np.zeros((num_samples, num_classes))
    for i in range(num_samples):
        node_class = Y[i]  # get class of node i
        id_cl = np.where(Cl == node_class)[0]
        if node_class>0:
            # if labeled, one-hot encoding
            y[i,id_cl-1] = 1
            
    if verbose:
        print(f"y = {y}")
        
    l_idx = np.where(Y >= 1)[0]
    u_idx = np.where(Y == 0)[0]
    
    if verbose:
        print(f"l_idx = {l_idx}")
        print(f"u_idx = {u_idx}")

    gamma = params['laplacian_regularization']
    Q = L + gamma * np.identity(num_samples)
    
    if not soft:    
        """
        Compute hard HFS.  

        f_l = solution for labeled data. 
        f_u = solution for unlabeled data
        f   = solution for all data
        """
        
        # compute L_uu, L_ul, L_ll
        Q_uu = Q[u_idx,:][:,u_idx]
        L_ul = L[u_idx,:][:,l_idx]
        
        if verbose:
            print(f"L = {L}")
            print(f"Q_uu = {Q_uu}")
            print(f"L_ul = {L_ul}")
        
        # compute f_u and f_l
        f_l = y[l_idx]
        f_u = np.linalg.inv(Q_uu) @ (L_ul @ f_l)
        
        if verbose:
            print(f"f_l = {f_l}")
            print(f"f_u = {f_u}")
            
        f = np.zeros((num_samples, num_classes))
        f[l_idx] = f_l
        f[u_idx] = f_u
        
        labels = np.zeros(num_samples)
        idx_classes = np.argmax(np.abs(f), axis=1)
        if verbose:
            print(f"idx_classes = {idx_classes}")
        for i in range(len(f)):
            labels[i] = Cl[idx_classes[i]+1]

    else:
        """
        Compute soft HFS.
        f = harmonic function solution 
        C = (n x n) diagonal matrix with c_l for labeled samples and c_u otherwise    
        """

        if verbose:
            print(f"params = {params}")
            print(f"c_l = {params['c_l']}")
            print(f"c_u = {params['c_u']}")
            
        c_l = params['c_l']
        c_u = params['c_u']
        
        C = np.zeros((num_samples, num_samples))
        for i in l_idx:
            C[i,i] = c_l
        for i in u_idx:
            C[i,i] = c_u
            
        if verbose:
            print(f"C = {C}")
        
        f = np.linalg.inv(np.linalg.inv(C) @ Q + np.identity(num_samples)) @ y
        
        labels = np.zeros(num_samples)
        idx_classes = np.argmax(np.abs(f), axis=1)
        if verbose:
            print(f"idx_classes = {idx_classes}")
        for i in range(len(f)):
            labels[i] = Cl[idx_classes[i]+1]
        
    """
    return the labels assignment from the hfs solution, and the solution f
    labels: (n x 1) class assignments [1,2,...,num_classes]    
    f : harmonic function solution
    """
    return labels, f


# # debug on toy graph

# L = np.array([[2,-1,-1,0],[-1,3,-1,-1],[-1,-1,2,0],[0,-1,0,1]])
# Y = np.array([1,0,0,2])
# soft = False

# compute_hfs(L,Y,soft=True, c_l=3.0, c_u=0.5, laplacian_regularization=0.0, verbose=True)


def two_moons_hfs(l=4, l_noisy=1, soft=False, dataset='data_2moons_hfs.mat', plot=True, seed=None, **params):
    """    
    TO BE COMPLETED.

    HFS for two_moons data.
    
    Parameters
    ----------
    l : int
        Number of labeled (unmasked) nodes provided to the HFS algorithm.
    l_noisy : int
        Number of *noisy* labels to introduce.
    soft : bool
        If true, use soft HFS, otherwise use hard HFS
    dataset : {'data_2moons_hfs.mat' or 'data_2moons_hfs_large.mat'}
        Which dataset to use.
    plot : bool
        If True, show plots
    seed : int
        If not None, set global numpy seed before choosing labels to reveal.
    """
    if seed is not None:
        np.random.seed(seed)

    # Load the data. At home, try to use the larger dataset.    
    in_data = loadmat(os.path.join('data', dataset))
    X = in_data['X']
    Y = np.array(in_data['Y'].squeeze(), dtype=np.uint32)

    # infer number of labels from samples
    num_samples = np.size(Y, 0)
    unique_classes = np.unique(Y)
    num_classes = len(unique_classes)
    
    # mask labels
    Y_masked = mask_labels(Y, l)
    assert len(np.unique(Y_masked)) > 2, "only one class in training data!"
    # introduce noise
    noise_indices = np.where(Y_masked == 0)[0]
    np.random.shuffle(noise_indices)
    noise_indices = noise_indices[:l_noisy]
    Y_masked[noise_indices] = np.random.choice(unique_classes, l_noisy)

    """
    compute hfs solution using either soft_hfs or hard_hfs
    """
    # Build graph Laplacian using the parameters:
    # params['laplacian_regularization'], params['var'], params['eps'], 
    # params['k'] and params['laplacian_normalization'].
    
    # build similarity graph
    W = build_similarity_graph(X, var=params.get('var'), eps=params.get('eps'), k=params.get('k'))
    L = build_laplacian(W, params.get('laplacian_normalization'))  

    labels, f = compute_hfs(L, Y_masked, soft, **params)
    # print(f"f = {f}")

    # Visualize results
    if plot:
        plot_classification(X, Y, Y_masked, noise_indices, labels, params['var'], params['eps'], params['k'])
    accuracy = np.mean(labels == np.squeeze(Y))
    print(f"Soft={soft}, Accuracy={accuracy:.3f}")
    return X, Y, labels, accuracy





"""
Define parameters for HFS
"""
params = {}

# regularization parameter (gamma)
params['laplacian_regularization'] = 0.0

# the sigma value for the exponential (similarity) function, already squared
params['var'] = 1.0

# Threshold eps for epsilon graphs
params['eps'] = None

# Number of neighbours k for k-nn. If zero, use epsilon-graph
params['k'] = 5

# String selecting which version of the laplacian matrix to construct.
# 'unn':  unnormalized, 'sym': symmetric normalization, 'rw':  random-walk normalization 
params['laplacian_normalization'] = 'unn'

# Coefficients for C matrix for soft HFS
params['c_l'] = 3.0
params['c_u'] = 1.0


seed = 42
X, Y, hard_labels, hard_accuracy = two_moons_hfs(l=10, l_noisy=0, soft=False, dataset='data_2moons_hfs.mat',
                                                 plot=True, seed=seed, **params)





np.random.seed(42)

try:
    for ii in range(20):
        X, Y, hard_labels, hard_accuracy = two_moons_hfs(l=4, l_noisy=0, soft=False, 
                                                        dataset='data_2moons_hfs_large.mat',
                                                        plot=False, seed=None, **params)
except:
    print(f"Training failed with l=4")
    pass
# mask_labels?  # check parameters


np.random.seed(42)
for ii in range(20):
    X, Y, hard_labels, hard_accuracy = two_moons_hfs(l=10, l_noisy=0, soft=False, 
                                                     dataset='data_2moons_hfs_large.mat',
                                                     plot=False, seed=None, **params)
    
# mask_labels?  # check parameters








"""
Define parameters for HFS
"""
params = {}

# regularization parameter (gamma)
params['laplacian_regularization'] = 0.0

# the sigma value for the exponential (similarity) function, already squared
params['var'] = 1.0

# Threshold eps for epsilon graphs
params['eps'] = None

# Number of neighbours k for k-nn. If zero, use epsilon-graph
params['k'] = 5

# String selecting which version of the laplacian matrix to construct.
# 'unn':  unnormalized, 'sym': symmetric normalization, 'rw':  random-walk normalization 
params['laplacian_normalization'] = 'unn'

# Coefficients for C matrix for soft HFS
params['c_l'] = 10.0
params['c_u'] = 1.0


# Comparing
seed = 42  # To run several times with random outcomes, set seed=None. Otherwise, set a seed for reproducibility.
plot = True 
dataset = 'data_2moons_hfs.mat' # Try also 'data_2moons_hfs_large.mat'

X, Y, hard_labels, hard_accuracy = two_moons_hfs(l=10, l_noisy=5, soft=False, dataset=dataset,
                                                 plot=plot, seed=seed, **params)
X, Y, soft_labels, soft_accuracy = two_moons_hfs(l=10, l_noisy=5, soft=True, dataset=dataset,
                                                 plot=plot, seed=seed, **params)


# Two Moons large dataset

# Comparing
seed = 42  # To run several times with random outcomes, set seed=None. Otherwise, set a seed for reproducibility.
plot = True 
dataset = 'data_2moons_hfs_large.mat'

X, Y, hard_labels, hard_accuracy = two_moons_hfs(l=10, l_noisy=5, soft=False, dataset=dataset,
                                                 plot=plot, seed=seed, **params)
X, Y, soft_labels, soft_accuracy = two_moons_hfs(l=10, l_noisy=5, soft=True, dataset=dataset,
                                                 plot=plot, seed=seed, **params)











import matplotlib.pyplot as plt
from imageio import imread
import numpy as np
import cv2
import os

from load_images import load_image_data, plot_image_data
from load_images import load_image_data_augmented, plot_image_data_augmented


# Function to preprocess the images
# You may try to change it and check the impact on the classification accuracy
def preprocess_image(image):
    """
    Parameters
    ----------
    image : array
        (width, height) array representing a grayscale image
    
    Returns
    -------
        (96, 96) preprocessed image
    """
    output_frame_size = 96   # do not change the output frame size!
    image = cv2.bilateralFilter(image, 9, 75, 75)
    image = cv2.equalizeHist(image)
    image = cv2.GaussianBlur(image, (5, 5), 0)
    im = cv2.resize(image, (output_frame_size, output_frame_size)).astype(float) # np.float is deprecated
    im -= im.mean()
    im /= im.max()
    image = im
    return image





# 10 images per person
np.random.seed(456)   # set seed, since labels are masked randomly
images, labels, masked_labels = load_image_data(preprocess_image)

# # 50 images per person
# images_a, labels_a, masked_labels_a = load_image_data_augmented(preprocess_image)
# plot_image_data_augmented(images_a)

# Uncomment below if you want to visualize the images
plot_image_data(images)
print(images.shape)
print(masked_labels.reshape(-1, 10))





"""
Define parameters for face recognition with HFS
"""
params_face_rec = {}
params_face_rec['laplacian_regularization'] = 1.0
params_face_rec['var'] = 10000.0
params_face_rec['eps'] = None
params_face_rec['k'] = 3
params_face_rec['laplacian_normalization'] = 'unn'
params_face_rec['c_l'] = None
params_face_rec['c_u'] = None


# graph Laplacian
L = build_laplacian_regularized(images, 
                                params_face_rec['laplacian_regularization'], 
                                params_face_rec['var'], 
                                params_face_rec['eps'], 
                                params_face_rec['k'], 
                                params_face_rec['laplacian_normalization'])


# Run HFS
predicted_labels, f = compute_hfs(L, masked_labels, soft=False, **params_face_rec)
accuracy = np.equal(predicted_labels, labels).mean()
print("Accuracy = ", accuracy)

# print(masked_labels)
# print(predicted_labels)
# print(labels)

# Visualize predicted vs true labels
plt.subplot(121)
plt.imshow(labels.reshape((-1, 10)))
plt.subplot(122)
plt.imshow(predicted_labels.reshape((-1, 10)))
plt.title("Accuracy: {}".format(accuracy))
plt.show()





"""
Define parameters for face recognition with HFS
"""
params_face_rec = {}
params_face_rec['laplacian_regularization'] = 1.0
params_face_rec['var'] = 10000.0
params_face_rec['eps'] = None
params_face_rec['k'] = 3
params_face_rec['laplacian_normalization'] = 'unn'
params_face_rec['c_l'] = 2.0
params_face_rec['c_u'] = 1.0


# graph Laplacian
L = build_laplacian_regularized(images, 
                                params_face_rec['laplacian_regularization'], 
                                params_face_rec['var'], 
                                params_face_rec['eps'], 
                                params_face_rec['k'], 
                                params_face_rec['laplacian_normalization'])


# Run HFS
predicted_labels, f = compute_hfs(L, masked_labels, soft=True, **params_face_rec)
accuracy = np.equal(predicted_labels, labels).mean()
print("Accuracy = ", accuracy)

# print(masked_labels)
# print(predicted_labels)
# print(labels)
# Visualize predicted vs true labels
plt.subplot(121)
plt.imshow(labels.reshape((-1, 10)))
plt.subplot(122)
plt.imshow(predicted_labels.reshape((-1, 10)))
plt.title("Accuracy: {}".format(accuracy))
plt.show()





np.random.seed(456)   # set seed, since labels are masked randomly

# 50 images per person
images_a, labels_a, masked_labels_a = load_image_data_augmented(preprocess_image)
plot_image_data_augmented(images_a)

# Uncomment below if you want to visualize the images
# plot_image_data(images_a) # bug
print(images_a.shape)
print(masked_labels_a.reshape(-1, 10))





"""
Define parameters for face recognition with HFS
"""
params_face_rec = {}
params_face_rec['laplacian_regularization'] = 1.0
params_face_rec['var'] = 10000.0
params_face_rec['eps'] = None
params_face_rec['k'] = 3
params_face_rec['laplacian_normalization'] = 'rw'
params_face_rec['c_l'] = None
params_face_rec['c_u'] = None


# graph Laplacian
L_a = build_laplacian_regularized(images_a, 
                                params_face_rec['laplacian_regularization'], 
                                params_face_rec['var'], 
                                params_face_rec['eps'], 
                                params_face_rec['k'], 
                                params_face_rec['laplacian_normalization'])


# Run HFS
predicted_labels_a, f = compute_hfs(L_a, masked_labels_a, soft=False, **params_face_rec)
accuracy = np.equal(predicted_labels_a, labels_a).mean()
print("Accuracy = ", accuracy)

# print(masked_labels)
# print(predicted_labels)
# print(labels)

# Visualize predicted vs true labels
plt.subplot(121)
plt.imshow(labels_a.reshape((-1, 10)))
plt.subplot(122)
plt.imshow(predicted_labels_a.reshape((-1, 10)))
plt.title("Accuracy: {}".format(accuracy))
plt.show()





"""
Define parameters for face recognition with HFS
"""
params_face_rec = {}
params_face_rec['laplacian_regularization'] = 1.0
params_face_rec['var'] = 10000.0
params_face_rec['eps'] = None
params_face_rec['k'] = 6
params_face_rec['laplacian_normalization'] = 'rw'
params_face_rec['c_l'] = 3.0
params_face_rec['c_u'] = 1.0


# graph Laplacian
L_a = build_laplacian_regularized(images_a, 
                                params_face_rec['laplacian_regularization'], 
                                params_face_rec['var'], 
                                params_face_rec['eps'], 
                                params_face_rec['k'], 
                                params_face_rec['laplacian_normalization'])


# Run HFS
predicted_labels_a, f = compute_hfs(L_a, masked_labels_a, soft=True, **params_face_rec)
accuracy = np.equal(predicted_labels_a, labels_a).mean()
print("Accuracy = ", accuracy)

# print(masked_labels)
# print(predicted_labels)
# print(labels)

# Visualize predicted vs true labels
plt.subplot(121)
plt.imshow(labels_a.reshape((-1, 10)))
plt.subplot(122)
plt.imshow(predicted_labels_a.reshape((-1, 10)))
plt.title("Accuracy: {}".format(accuracy))
plt.show()














import matplotlib.pyplot as plt
import numpy as np
import cv2 as cv
import os
import sys
from scipy.spatial import distance
import scipy.io as sio

from helper_online_ssl import create_user_profile, online_face_recognition


"""
Define parameters for face recognition with HFS
"""
params_online_ssl = {}
params_online_ssl['laplacian_regularization'] = 1.0
params_online_ssl['var'] = 10000.0
params_online_ssl['eps'] = None
params_online_ssl['k'] = 5
params_online_ssl['laplacian_normalization'] = 'unn'
params_online_ssl['c_l'] = 2.0
params_online_ssl['c_u'] = 1.0


class IncrementalKCenters:
    def __init__(self, labeled_faces, labels, label_names, max_num_centroids=5):
        #  Number of labels
        self.n_labels = max(labels)

        #  Dimension of the input image
        self.image_dimension = labeled_faces.shape[1]

        #  Check input validity
        assert (set(labels) == set(
            range(1, 1 + self.n_labels))), "Initially provided faces should be labeled in [1, max]"
        assert (len(labeled_faces) == len(labels)), "Initial faces and initial labels are not of same size"

        #  Number of labelled faces
        self.n_labeled_faces = len(labeled_faces)

        # Model parameter : number of maximum stored centroids
        self.max_num_centroids = max_num_centroids

        # Model centroids (inital labeled faces). Shape = (number_of_centroids, dimension)
        self.centroids = labeled_faces

        # Centroids labels
        self.Y = labels
        
        # Label names (= user names)
        self.label_names = label_names

        # Variables that are initialized in online_ssl_update_centroids()
        self.centroids_distances = None
        self.taboo = None
        self.V = None
        self.init = True

        # index of x_t (initialized later)
        self.last_face = None
    
    def initialize(self):
        """
        Initialization after the first time that the maximum number of centroids is reached.
        """       
        #  Compute the centroids distances
        self.centroids_distances = distance.cdist(self.centroids, self.centroids)

        #  set labeled nodes and self loops as infinitely distant, to avoid merging labeled centroids
        np.fill_diagonal(self.centroids_distances, +np.inf)
        self.centroids_distances[0:self.n_labeled_faces, 0:self.n_labeled_faces] = +np.inf

        # put labeled nodes in the taboo list
        self.taboo = np.array(range(self.centroids.shape[0])) < self.n_labeled_faces

        # initialize multiplicity
        self.V = np.ones(self.centroids.shape[0])


    def online_ssl_update_centroids(self, face):
        """
        TO BE COMPLETED

        Update centroids, multiplicity vector V, labels Y.
        
        Note: In Y, set label to 0 for unlabeled faces.

        Parameters
        ----------
        face : array
            New sample
        
        Returns
        --------
        None
        """

        assert (self.image_dimension == len(face)), "new image not of good size"

        # Case 1: maximum number of centroids has been reached.
        if self.centroids.shape[0] >= self.max_num_centroids + 1:
            if self.init:
                #  Initialization after the first time that the maximum number of centroids is reached
                self.initialize()
                self.init = False
            """
            Find c_rep and c_add following Algorithm 1.
            
            - c_1, c_2 = two closest centroids (minimum distance) such that at least one of them is not in self.taboo.
            - c_rep = centroid in {c_1, c_2} that is in self.taboo. If none of them is in self.taboo, c_rep is the one
                      with largest multiplicity.
            - c_add = centroid in {c_1, c_2} that is not c_rep.
            """
            
            # compute pairs of distances between centroids
            pairs_distances = distance.cdist(self.centroids, self.centroids)
            print(f"pairs_distances = {pairs_distances}")
            # reshape pairs of distances as a list and sort it in ascending order for loop
            list_pairs_distances = np.reshape(pairs_distances, (-1,1))
            sorted_list_pairs_distances = sorted(list(list_pairs_distances))
            
            # loop, starting from min distance, till finding an eligible pair of centroids
            print(f"self.taboo = {self.taboo}")
            print(f"sorted_list_pairs_distances = {sorted_list_pairs_distances}")
            for d in sorted_list_pairs_distances:
                i1, i2 = np.where(pairs_distances == d)
                c_1 = self.centroids[i1]
                c_2 = self.centroids[i2]
                print(f"------")
                print(f"d = {d}")
                print(f"i1 = {i1}")
                print(f"i2 = {i2}")
                print(f"self.taboo(i1) = {self.taboo[i1]}")
                print(f"self.taboo(i2) = {self.taboo[i2]}")
                if self.taboo[i1] is True or self.taboo[i2] is True:
                    print(f"break")
                    break
                raise NameError("could not find eligible centroids - all are in self.taboo")
                
            if self.taboo[i1] is False and self.taboo[i2] is False:
                m1 = self.V[i1]
                m2 = self.V[i2]
                if m1 < m2 : 
                    c_rep = c_2
                    c_add = c_1
                else:
                    c_rep = c_1
                    c_add = c_2
            else:
                if self.taboo[i1] is True:
                    c_rep = c_1
                    c_add = c_2
                else:
                    c_rep = c_2
                    c_add = c_1
                
            """
            Update data structures: self.centroids and self.V
            """
            # (re)find multiplicities
            m1 = self.V[i1]
            m2 = self.V[i2]
            # assign c_rep to centroid of highest multiplicity, update V accordingly
            if m1 > m2:
                self.centroids[i1] = c_rep
                self.V[i1] = m1 + m2
                self.centroids[i2] = c_add
                self.V[i2] = 1
            else:
                self.centroids[i2] = c_rep
                self.V[i2] = m1 + m2
                self.centroids[i1] = c_add
                self.V[i1] = 1

            """
            Update the matrix containing the distances.
            """
            dist_row = distance.cdist(np.array([self.centroids[c_add]]), self.centroids)[0]
            dist_row[c_add] = +np.inf
            self.centroids_distances[c_add, :] = dist_row
            self.centroids_distances[:, c_add] = dist_row
            self.last_face = c_add

        # Case 2: create new centroid with face
        # Remark: the multiplicities vector self.V is initialized in self.initialize()
        else:
            current_len = len(self.centroids)
            self.Y = np.append(self.Y, 0)
            self.centroids = np.vstack([self.centroids, face])
            
            
            
            
    def online_ssl_compute_solution(self):
        """
        Returns a prediction corresponding to self.last_face.
        """

        # Multiplicity matrix
        if self.init:
            V = np.diag(np.ones(self.centroids.shape[0]))
            self.last_face = self.centroids.shape[0] - 1
        else:
            V = np.diag(self.V)
            
        # Build quantized graph and its regularized Laplacian
        
        W = V @ build_similarity_graph(self.centroids, params_online_ssl['var'], params_online_ssl['eps'],  params_online_ssl['k']) @ V
        L = build_laplacian(W, laplacian_normalization=params_online_ssl['laplacian_normalization'])
        Q = L + params['laplacian_regularization'] * np.identity(self.centroids.shape[0])  # regularized Laplacian

        # Compute the hard HFS solution f. 
        labels, f = compute_hfs(Q, self.Y, soft=False, **params_online_ssl)

        # Return the score for each possible label
        num_classes = len(np.unique(self.Y))-1 
        label_scores = []
        for ii in range(num_classes):
            label = self.label_names[ii]
            score = f[self.last_face, ii]
            label_scores.append((label, score))
        
        # handle unknown faces
        return [("unknown", score)]

        return label_scores


create_user_profile('ben')         # choose your names here :)
# create_user_profile('alice')


online_face_recognition(['ben'], IncrementalKCenters, n_pictures=3)



